{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a429347d",
   "metadata": {},
   "source": [
    "<center><h1>Model Performance Testing â€“ Feasibility Analysis</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfecb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../..\")))\n",
    "\n",
    "from utils.pricing_market_logic_multiproduct import (\n",
    "    get_profits,\n",
    "    get_monopoly_prices,\n",
    "    get_quantities,\n",
    ")\n",
    "from utils.prompts import PP_P0, GENERAL_PROMPT\n",
    "from utils.utils import has_converged_to_price\n",
    "\n",
    "from helper_functions import create_output_paths, save_round_data, update_plot\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "API_KEY = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82eb25",
   "metadata": {},
   "source": [
    "# Parameters definition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA, MU, BETA, SIGMA = 1, 0.25, 100, 0  # Follows Calvano et al. (2020b)\n",
    "C_i, A_i, A_0 = 1, 2, 0\n",
    "N_FIRMS, MG_C = 1, 1.0\n",
    "WILLIWGNES_TO_PAY = 4.51 * ALPHA\n",
    "A = tuple([A_i for _ in range(N_FIRMS)])\n",
    "ALPHA = tuple([ALPHA for _ in range(N_FIRMS)])\n",
    "C = tuple([C_i for _ in range(N_FIRMS)])\n",
    "group_idxs = tuple([i for i in range(1, N_FIRMS + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9cd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricingSchema(BaseModel):\n",
    "    observations_and_thoughts: str\n",
    "    plans: str\n",
    "    insights: str\n",
    "    price: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c41162",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_m = get_monopoly_prices(\n",
    "    a0=A_0,\n",
    "    a=A,\n",
    "    mu=MU,\n",
    "    alpha=ALPHA,\n",
    "    c=C,\n",
    "    multiplier=BETA,\n",
    "    sigma=SIGMA,\n",
    "    group_idxs=group_idxs,\n",
    ")\n",
    "\n",
    "q_m = get_quantities(\n",
    "    p=tuple(p_m),\n",
    "    a0=A_0,\n",
    "    a=A,\n",
    "    mu=MU,\n",
    "    alpha=ALPHA,\n",
    "    multiplier=BETA,\n",
    "    sigma=SIGMA,\n",
    "    group_idxs=group_idxs,\n",
    ")\n",
    "\n",
    "pi_m = get_profits(\n",
    "    p=tuple(p_m),\n",
    "    c=C,\n",
    "    a0=A_0,\n",
    "    a=A,\n",
    "    mu=MU,\n",
    "    alpha=ALPHA,\n",
    "    multiplier=BETA,\n",
    "    sigma=SIGMA,\n",
    "    group_idxs=group_idxs,\n",
    ")\n",
    "print(f\"Monopoly prices: {p_m} | Quantities: {q_m} | Profits: {pi_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6135552",
   "metadata": {},
   "source": [
    "# Models testing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f99c7a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad208b94",
   "metadata": {},
   "source": [
    "![Models Tested](imgs/models_and_sizes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7d4f3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST = [  # \"google/gemma-3-1b\",\n",
    "    #   \"llama-3.2-1b-instruct\",\n",
    "    #   \"deepseek-ai.deepseek-r1-distill-qwen-1.5b\",\n",
    "    #   \"microsoft/phi-4-mini-reasoning\",\n",
    "    #   \"google/gemma-3-4b\",\n",
    "    #   \"mistralai_-_mistral-7b-instruct-v0.2\",\n",
    "    #   \"mistralai/mistral-7b-instruct-v0.3\",\n",
    "    #   \"deepseek/deepseek-r1-0528-qwen3-8b\",\n",
    "    #   \"qwen/qwen3-8b\",\n",
    "    #   \"deepseek-r1-distill-qwen-7b\",\n",
    "    #   \"deepseek-ai.deepseek-r1-distill-llama-8b\",\n",
    "    #   \"meta-llama-3.1-8b-instruct\",\n",
    "    \"google/gemma-2-9b\",\n",
    "    #   \"qwen/qwen2.5-vl-7b\",\n",
    "    #   \"mistralai_-_mistral-nemo-instruct-2407\",\n",
    "    #   \"google/gemma-3-12b\",\n",
    "    #   \"microsoft/phi-4-reasoning-plus\",\n",
    "    #   \"deepseek-ai.deepseek-r1-distill-qwen-14b\",\n",
    "    #   \"mistralai/magistral-small\",\n",
    "    #   \"deepseek-ai.deepseek-r1-distill-qwen-32b\"\n",
    "]\n",
    "\n",
    "for MODEL_NAME in MODEL_LIST:\n",
    "    # Connect to LM Studio\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "    paths = create_output_paths(MODEL_NAME)\n",
    "\n",
    "    # Create figure and subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plans, insights, market_data = (\n",
    "        \"No previous plans.\",\n",
    "        \"No previous insights.\",\n",
    "        \"No previous market data.\",\n",
    "    )\n",
    "    price_history, quantity_history, profit_history, time_history = [], [], [], []\n",
    "    for i in range(1, 10 + 1):\n",
    "        prompt = GENERAL_PROMPT.format(\n",
    "            marginal_cost=MG_C,\n",
    "            willigness_to_pay=WILLIWGNES_TO_PAY,\n",
    "            previous_plans=plans,\n",
    "            previous_insights=insights,\n",
    "            market_data=market_data,\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": PP_P0},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            response = client.beta.chat.completions.parse(\n",
    "                model=MODEL_NAME, messages=messages, response_format=PricingSchema\n",
    "            )\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            # Results\n",
    "            insights = result[\"insights\"]\n",
    "            observations = result[\"observations_and_thoughts\"]\n",
    "            plans = result[\"plans\"]\n",
    "            price = result[\"price\"]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to get response at round {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        quantity = get_quantities(\n",
    "            p=tuple([price]),\n",
    "            a0=A_0,\n",
    "            a=A,\n",
    "            mu=MU,\n",
    "            alpha=ALPHA,\n",
    "            multiplier=BETA,\n",
    "            sigma=SIGMA,\n",
    "            group_idxs=group_idxs,\n",
    "        )\n",
    "        profit = get_profits(\n",
    "            p=([price]),\n",
    "            c=C,\n",
    "            a0=A_0,\n",
    "            a=A,\n",
    "            mu=MU,\n",
    "            alpha=ALPHA,\n",
    "            multiplier=BETA,\n",
    "            sigma=SIGMA,\n",
    "            group_idxs=group_idxs,\n",
    "        )\n",
    "\n",
    "        price_history.append(price)\n",
    "        quantity_history.append(quantity)\n",
    "        profit_history.append(profit)\n",
    "        time_history.append(i)\n",
    "\n",
    "        market_data_result = f\"\"\"Round {i}:\\n \\t - My price: {price}\\n \\t - Quantity sold: {quantity[0]}\\n \\t - My profit earned: {profit[0]}\\n\"\"\"\n",
    "        market_data = save_round_data(\n",
    "            i, paths, insights, plans, observations, market_data_result\n",
    "        )\n",
    "\n",
    "        # Update plot\n",
    "        update_plot(\n",
    "            fig,\n",
    "            axs,\n",
    "            i,\n",
    "            p_m,\n",
    "            q_m,\n",
    "            pi_m,\n",
    "            price_history,\n",
    "            quantity_history,\n",
    "            profit_history,\n",
    "            time_history,\n",
    "            MODEL_NAME,\n",
    "            paths[\"start_time\"],\n",
    "            paths[\"plot\"],\n",
    "        )\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6cf4b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_results = {\n",
    "    \"model\": [\n",
    "        \"deepseek-ai.deepseek-r1-distill-qwen-32b\",\n",
    "        \"mistralai/magistral-small\",\n",
    "        \"deepseek-ai.deepseek-r1-distill-qwen-14b\",\n",
    "        \"microsoft/phi-4-reasoning-plus\",\n",
    "        \"google/gemma-3-12b\",\n",
    "        \"mistralai_-_mistral-nemo-instruct-2407\",\n",
    "        \"qwen/qwen2.5-vl-7b\",\n",
    "        \"google/gemma-2-9b\",\n",
    "        \"meta-llama-3.1-8b-instruct\",\n",
    "        \"deepseek-ai.deepseek-r1-distill-llama-8b\",\n",
    "        \"deepseek-r1-distill-qwen-7b\",\n",
    "        \"qwen/qwen3-8b\",\n",
    "        \"deepseek/deepseek-r1-0528-qwen3-8b\",\n",
    "        \"mistralai/mistral-7b-instruct-v0.3\",\n",
    "        \"mistralai_-_mistral-7b-instruct-v0.2\",\n",
    "        \"google/gemma-3-4b\",\n",
    "        \"microsoft/phi-4-mini-reasoning\",\n",
    "        \"deepseek-ai.deepseek-r1-distill-qwen-1.5b\",\n",
    "        \"llama-3.2-1b-instruct\",\n",
    "        \"google/gemma-3-1b\",\n",
    "    ],\n",
    "    \"minutes_taken\": [\n",
    "        19,\n",
    "        7,\n",
    "        3,\n",
    "        5,\n",
    "        6,\n",
    "        5,\n",
    "        3,\n",
    "        3,\n",
    "        3,\n",
    "        3,\n",
    "        2,\n",
    "        6,\n",
    "        3,\n",
    "        3,\n",
    "        3,\n",
    "        3,\n",
    "        2,\n",
    "        0.5,\n",
    "        0.5,\n",
    "        0.5,\n",
    "    ],\n",
    "    \"n_wrong_insights\": [3, 0, 7, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, -1, 0, 1],\n",
    "    \"n_wrong_obs\": [1, 0, 5, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 1],\n",
    "    \"n_wrong_plans\": [2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 8, 0, 2, -1, 8, 4],\n",
    "    \"completed_10_rounds\": [\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        False,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        False,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        True,\n",
    "    ],\n",
    "    \"model_size_gb\": [16, 13.5, 9, 8, 8, 7, 6, 6, 5, 5, 5, 5, 5, 4, 4, 3, 2, 2, 1, 0.7],\n",
    "    \"parameters\": [\n",
    "        \"32B\",\n",
    "        None,\n",
    "        \"14B\",\n",
    "        None,\n",
    "        None,\n",
    "        \"12B\",\n",
    "        \"7B\",\n",
    "        \"9B\",\n",
    "        \"8B\",\n",
    "        \"8B\",\n",
    "        \"7B\",\n",
    "        None,\n",
    "        None,\n",
    "        \"7B\",\n",
    "        \"7B\",\n",
    "        None,\n",
    "        None,\n",
    "        \"1.5B\",\n",
    "        \"1B\",\n",
    "        None,\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(all_runs_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231ae86",
   "metadata": {},
   "source": [
    "# Mistral AI Inference - Performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f151c",
   "metadata": {},
   "source": [
    "Mistral AI API improves performance from 35\" per answer (locally) to 5\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"magistral-small-2506-API\"\n",
    "MODEL_MISTRAL = MODEL_NAME.replace(\"-API\", \"\")\n",
    "print(f\"Using model: {MODEL_MISTRAL}\")\n",
    "\n",
    "client = Mistral(api_key=API_KEY)\n",
    "\n",
    "paths = create_output_paths(MODEL_NAME)\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plans, insights, market_data = (\n",
    "    \"No previous plans.\",\n",
    "    \"No previous insights.\",\n",
    "    \"No previous market data.\",\n",
    ")\n",
    "price_history, quantity_history, profit_history, time_history = [], [], [], []\n",
    "for i in range(1, 300 + 1):\n",
    "    prompt = GENERAL_PROMPT.format(\n",
    "        marginal_cost=MG_C,\n",
    "        willigness_to_pay=WILLIWGNES_TO_PAY,\n",
    "        previous_plans=plans,\n",
    "        previous_insights=insights,\n",
    "        market_data=market_data,\n",
    "    )\n",
    "\n",
    "    trial = 0\n",
    "\n",
    "    while trial <= 3:\n",
    "        try:\n",
    "            chat_response = client.chat.complete(\n",
    "                model=MODEL_MISTRAL,\n",
    "                stream=False,\n",
    "                temperature=0.7,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": PP_P0},\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"Respond only with a JSON object with this schema:\n",
    "                                    {\n",
    "                                    \"observations\": string,\n",
    "                                    \"plans\": string,\n",
    "                                    \"insights\": string,\n",
    "                                    \"chosen_price\": float\n",
    "                                    }\"\"\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            )\n",
    "            result = json.loads(chat_response.choices[0].message.content)\n",
    "            if isinstance(result, list):\n",
    "                result = result[0]\n",
    "\n",
    "            insights = result[\"insights\"]\n",
    "            observations = result[\"observations\"]\n",
    "            plans = result[\"plans\"]\n",
    "            price = result[\"chosen_price\"]\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to get response at round {i}: {e}\")\n",
    "            trial += 1\n",
    "\n",
    "        finally:\n",
    "            if trial >= 3:\n",
    "                print(f\"[ERROR] Failed to get response at round {i}\")\n",
    "                break\n",
    "\n",
    "    quantity = get_quantities(\n",
    "        p=tuple([price]),\n",
    "        a0=A_0,\n",
    "        a=A,\n",
    "        mu=MU,\n",
    "        alpha=ALPHA,\n",
    "        multiplier=BETA,\n",
    "        sigma=SIGMA,\n",
    "        group_idxs=group_idxs,\n",
    "    )\n",
    "    profit = get_profits(\n",
    "        p=([price]),\n",
    "        c=C,\n",
    "        a0=A_0,\n",
    "        a=A,\n",
    "        mu=MU,\n",
    "        alpha=ALPHA,\n",
    "        multiplier=BETA,\n",
    "        sigma=SIGMA,\n",
    "        group_idxs=group_idxs,\n",
    "    )\n",
    "\n",
    "    price_history.append(price)\n",
    "    quantity_history.append(quantity)\n",
    "    profit_history.append(profit)\n",
    "    time_history.append(i)\n",
    "\n",
    "    market_data_result = f\"\"\"Round {i}:\\n \\t - My price: {price}\\n \\t - Quantity sold: {quantity[0]}\\n \\t - My profit earned: {profit[0]}\\n\"\"\"\n",
    "    market_data = save_round_data(\n",
    "        i, paths, insights, plans, observations, market_data_result\n",
    "    )\n",
    "\n",
    "    # Update plot\n",
    "    update_plot(\n",
    "        fig,\n",
    "        axs,\n",
    "        i,\n",
    "        p_m,\n",
    "        q_m,\n",
    "        pi_m,\n",
    "        price_history,\n",
    "        quantity_history,\n",
    "        profit_history,\n",
    "        time_history,\n",
    "        MODEL_NAME,\n",
    "        paths[\"start_time\"],\n",
    "        paths[\"plot\"],\n",
    "    )\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6265a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_target = p_m[0]\n",
    "converged = has_converged_to_price(price_history, price_target)\n",
    "\n",
    "if converged:\n",
    "    print(f\"Prices converged to {price_target}\")\n",
    "else:\n",
    "    print(f\"Prices did NOT converge to {price_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df413c",
   "metadata": {},
   "source": [
    "# Results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3e69f",
   "metadata": {},
   "source": [
    "The models that passed the criteria at this stage are:\n",
    "- Deepseek R1 Distill QWEN 32B\n",
    "- MistralAI - Magistral Small\n",
    "- Google Gemma 2 9B\n",
    "- Deepseek R1 Qwen3 8B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f50e4",
   "metadata": {},
   "source": [
    "![Deepseek 32B](imgs/results_deepseek_r1_distill_qwen_32b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79995a57",
   "metadata": {},
   "source": [
    "![Deepseek 8B](imgs/results_deepseek_r1_qwen3_8b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534db47",
   "metadata": {},
   "source": [
    "![Gemma 2 9B](imgs/results_gemma_2_9b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edf1a3",
   "metadata": {},
   "source": [
    "![Magistral Small](imgs/results_magistral_small.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8214c9f",
   "metadata": {},
   "source": [
    "We will proceed using the MistralAI API to fasten experimentation as latency is highly reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5e585",
   "metadata": {},
   "source": [
    "![Magistral Small API](imgs/results_magistral_small_API.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithmic-pricing-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
