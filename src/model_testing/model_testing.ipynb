{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a429347d",
   "metadata": {},
   "source": [
    "<center><h1>Model Performance Testing â€“ Feasibility Analysis</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfecb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../..\")))\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import lmstudio as lms\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "\n",
    "\n",
    "from utils.pricing_market_logic_multiproduct import (get_profits,\n",
    "                                                     get_monopoly_prices,\n",
    "                                                     get_quantities\n",
    "                                                    )\n",
    "from utils.prompts import PP_P0, GENERAL_PROMPT\n",
    "\n",
    "from helper_functions import (create_output_paths,\n",
    "                                save_round_data,\n",
    "                                update_plot,\n",
    "                                get_last_100_rounds,\n",
    "                                has_converged_to_price\n",
    "                                )\n",
    "\n",
    "API_KEY = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82eb25",
   "metadata": {},
   "source": [
    "# Parameters definition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee74b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA, MU, BETA, SIGMA = 1, 0.25, 100, 0 #Follows Calvano et al. (2020b)\n",
    "C_i, A_i, A_0 = 1, 2, 0\n",
    "N_FIRMS, MG_C = 1, 1.0\n",
    "WILLIWGNES_TO_PAY = 4.51 * ALPHA\n",
    "A = tuple([A_i for _ in range(N_FIRMS)])\n",
    "ALPHA = tuple([ALPHA for _ in range(N_FIRMS)])\n",
    "C = tuple([C_i for _ in range(N_FIRMS)])\n",
    "group_idxs = tuple([i for i in range(1, N_FIRMS+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9cd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricingSchema(BaseModel):\n",
    "    observations_and_thoughts: str\n",
    "    plans: str\n",
    "    insights: str\n",
    "    price: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c41162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monopoly prices: [1.801985642429283] | Quantities: [68.82729299489286] | Profits: [55.19850078917764]\n"
     ]
    }
   ],
   "source": [
    "p_m = get_monopoly_prices(\n",
    "    a0=A_0,\n",
    "    a=A,\n",
    "    mu=MU,\n",
    "    alpha=ALPHA,\n",
    "    c=C,\n",
    "    multiplier=BETA,\n",
    "    sigma=SIGMA,\n",
    "    group_idxs=group_idxs,\n",
    ")\n",
    "\n",
    "q_m = get_quantities(\n",
    "    p=tuple(p_m),\n",
    "    a0=A_0,\n",
    "    a=A,\n",
    "    mu=MU,\n",
    "    alpha=ALPHA,\n",
    "    multiplier=BETA,\n",
    "    sigma=SIGMA,\n",
    "    group_idxs=group_idxs,\n",
    ")\n",
    "\n",
    "pi_m = get_profits(\n",
    "    p=tuple(p_m),\n",
    "    c=C,\n",
    "    a0=A_0,\n",
    "    a=A,\n",
    "    mu=MU,\n",
    "    alpha=ALPHA,\n",
    "    multiplier=BETA,\n",
    "    sigma=SIGMA,\n",
    "    group_idxs=group_idxs,\n",
    ")\n",
    "print(f\"Monopoly prices: {p_m} | Quantities: {q_m} | Profits: {pi_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6135552",
   "metadata": {},
   "source": [
    "# Models testing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f99c7a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad208b94",
   "metadata": {},
   "source": [
    "![Models Tested](imgs/models_and_sizes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7d4f3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST = [#\"google/gemma-3-1b\",\n",
    "            #   \"llama-3.2-1b-instruct\",\n",
    "            #   \"deepseek-ai.deepseek-r1-distill-qwen-1.5b\",\n",
    "            #   \"microsoft/phi-4-mini-reasoning\",\n",
    "            #   \"google/gemma-3-4b\",\n",
    "            #   \"mistralai_-_mistral-7b-instruct-v0.2\",\n",
    "            #   \"mistralai/mistral-7b-instruct-v0.3\",\n",
    "            #   \"deepseek/deepseek-r1-0528-qwen3-8b\",\n",
    "            #   \"qwen/qwen3-8b\",\n",
    "            #   \"deepseek-r1-distill-qwen-7b\",\n",
    "            #   \"deepseek-ai.deepseek-r1-distill-llama-8b\",\n",
    "            #   \"meta-llama-3.1-8b-instruct\",\n",
    "              \"google/gemma-2-9b\",\n",
    "            #   \"qwen/qwen2.5-vl-7b\",\n",
    "            #   \"mistralai_-_mistral-nemo-instruct-2407\",\n",
    "            #   \"google/gemma-3-12b\",\n",
    "            #   \"microsoft/phi-4-reasoning-plus\",\n",
    "            #   \"deepseek-ai.deepseek-r1-distill-qwen-14b\",\n",
    "            #   \"mistralai/magistral-small\",\n",
    "            #   \"deepseek-ai.deepseek-r1-distill-qwen-32b\"\n",
    "              ]\n",
    "\n",
    "for MODEL_NAME in MODEL_LIST:\n",
    "\n",
    "    # Connect to LM Studio\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "    paths = create_output_paths(MODEL_NAME)\n",
    "    plans, insights, market_data = \"No previous plans.\", \"No previous insights.\", \"No previous market data.\"\n",
    "\n",
    "    # Create figure and subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    price_history, quantity_history, profit_history, time_history = [], [], [], []\n",
    "    for i in range(1, 10+1):\n",
    "\n",
    "        prompt = GENERAL_PROMPT.format(marginal_cost=MG_C,\n",
    "                                        willigness_to_pay=WILLIWGNES_TO_PAY,\n",
    "                                        previous_plans = plans,\n",
    "                                        previous_insights = insights,\n",
    "                                        market_data = market_data\n",
    "                                        )\n",
    "        messages =[{'role': 'system', 'content': PP_P0},\n",
    "                    {'role': 'user', 'content': prompt},\n",
    "                ]\n",
    "                                    \n",
    "        try:\n",
    "            response = client.beta.chat.completions.parse(\n",
    "                            model=MODEL_NAME,\n",
    "                            messages=messages,\n",
    "                            response_format=PricingSchema\n",
    "                        )\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            #Results \n",
    "            insights = result['insights']\n",
    "            observations = result['observations_and_thoughts']\n",
    "            plans = result['plans']\n",
    "            price = result['price']\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to get response at round {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        quantity = get_quantities(p=tuple([price]),\n",
    "                                    a0=A_0,\n",
    "                                    a=A,\n",
    "                                    mu=MU,\n",
    "                                    alpha=ALPHA,\n",
    "                                    multiplier=BETA,\n",
    "                                    sigma=SIGMA,\n",
    "                                    group_idxs=group_idxs,\n",
    "                                )\n",
    "        profit = get_profits(\n",
    "                                p=([price]),\n",
    "                                c=C,\n",
    "                                a0=A_0,\n",
    "                                a=A,\n",
    "                                mu=MU,\n",
    "                                alpha=ALPHA,\n",
    "                                multiplier=BETA,\n",
    "                                sigma=SIGMA,\n",
    "                                group_idxs=group_idxs,\n",
    "                            )\n",
    "\n",
    "\n",
    "        price_history.append(price)\n",
    "        quantity_history.append(quantity)\n",
    "        profit_history.append(profit)\n",
    "        time_history.append(i)\n",
    "\n",
    "        market_data_result = f\"\"\"Round {i}:\\n \\t - My price: {price}\\n \\t - Quantity sold: {quantity[0]}\\n \\t - My profit earned: {profit[0]}\\n\"\"\"\n",
    "        market_data = save_round_data(i, paths, insights, plans, observations, market_data_result)\n",
    "\n",
    "        #Update plot\n",
    "        update_plot(fig, axs, i, p_m, q_m, pi_m, price_history, quantity_history, \n",
    "                    profit_history, time_history, MODEL_NAME, paths[\"start_time\"], paths[\"plot\"])\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6cf4b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2862bcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "minutes_taken",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n_wrong_insights",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_wrong_obs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_wrong_plans",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "completed_10_rounds",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "model_size_gb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "parameters",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "6102aebd-c455-47a4-87d8-5fa644399533",
       "rows": [
        [
         "0",
         "deepseek-ai.deepseek-r1-distill-qwen-32b",
         "19.0",
         "3",
         "1",
         "2",
         "True",
         "16.0",
         "32B"
        ],
        [
         "1",
         "mistralai/magistral-small",
         "7.0",
         "0",
         "0",
         "0",
         "True",
         "13.5",
         null
        ],
        [
         "2",
         "deepseek-ai.deepseek-r1-distill-qwen-14b",
         "3.0",
         "7",
         "5",
         "0",
         "True",
         "9.0",
         "14B"
        ],
        [
         "3",
         "microsoft/phi-4-reasoning-plus",
         "5.0",
         "1",
         "1",
         "3",
         "False",
         "8.0",
         null
        ],
        [
         "4",
         "google/gemma-3-12b",
         "6.0",
         "0",
         "0",
         "0",
         "True",
         "8.0",
         null
        ],
        [
         "5",
         "mistralai_-_mistral-nemo-instruct-2407",
         "5.0",
         "0",
         "0",
         "0",
         "True",
         "7.0",
         "12B"
        ],
        [
         "6",
         "qwen/qwen2.5-vl-7b",
         "3.0",
         "0",
         "0",
         "0",
         "True",
         "6.0",
         "7B"
        ],
        [
         "7",
         "google/gemma-2-9b",
         "3.0",
         "0",
         "0",
         "0",
         "True",
         "6.0",
         "9B"
        ],
        [
         "8",
         "meta-llama-3.1-8b-instruct",
         "3.0",
         "1",
         "0",
         "0",
         "True",
         "5.0",
         "8B"
        ],
        [
         "9",
         "deepseek-ai.deepseek-r1-distill-llama-8b",
         "3.0",
         "0",
         "0",
         "0",
         "True",
         "5.0",
         "8B"
        ],
        [
         "10",
         "deepseek-r1-distill-qwen-7b",
         "2.0",
         "0",
         "1",
         "1",
         "True",
         "5.0",
         "7B"
        ],
        [
         "11",
         "qwen/qwen3-8b",
         "6.0",
         "0",
         "0",
         "0",
         "False",
         "5.0",
         null
        ],
        [
         "12",
         "deepseek/deepseek-r1-0528-qwen3-8b",
         "3.0",
         "0",
         "0",
         "0",
         "True",
         "5.0",
         null
        ],
        [
         "13",
         "mistralai/mistral-7b-instruct-v0.3",
         "3.0",
         "0",
         "0",
         "0",
         "True",
         "4.0",
         "7B"
        ],
        [
         "14",
         "mistralai_-_mistral-7b-instruct-v0.2",
         "3.0",
         "5",
         "0",
         "8",
         "True",
         "4.0",
         "7B"
        ],
        [
         "15",
         "google/gemma-3-4b",
         "3.0",
         "0",
         "0",
         "0",
         "True",
         "3.0",
         null
        ],
        [
         "16",
         "microsoft/phi-4-mini-reasoning",
         "2.0",
         "0",
         "0",
         "2",
         "False",
         "2.0",
         null
        ],
        [
         "17",
         "deepseek-ai.deepseek-r1-distill-qwen-1.5b",
         "0.5",
         "-1",
         "-1",
         "-1",
         "False",
         "2.0",
         "1.5B"
        ],
        [
         "18",
         "llama-3.2-1b-instruct",
         "0.5",
         "0",
         "0",
         "8",
         "False",
         "1.0",
         "1B"
        ],
        [
         "19",
         "google/gemma-3-1b",
         "0.5",
         "1",
         "1",
         "4",
         "True",
         "0.7",
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>minutes_taken</th>\n",
       "      <th>n_wrong_insights</th>\n",
       "      <th>n_wrong_obs</th>\n",
       "      <th>n_wrong_plans</th>\n",
       "      <th>completed_10_rounds</th>\n",
       "      <th>model_size_gb</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deepseek-ai.deepseek-r1-distill-qwen-32b</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistralai/magistral-small</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>13.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-ai.deepseek-r1-distill-qwen-14b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>microsoft/phi-4-reasoning-plus</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google/gemma-3-12b</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistralai_-_mistral-nemo-instruct-2407</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qwen/qwen2.5-vl-7b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google/gemma-2-9b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meta-llama-3.1-8b-instruct</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deepseek-ai.deepseek-r1-distill-llama-8b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deepseek-r1-distill-qwen-7b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>qwen/qwen3-8b</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deepseek/deepseek-r1-0528-qwen3-8b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mistralai/mistral-7b-instruct-v0.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mistralai_-_mistral-7b-instruct-v0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google/gemma-3-4b</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>microsoft/phi-4-mini-reasoning</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deepseek-ai.deepseek-r1-distill-qwen-1.5b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>llama-3.2-1b-instruct</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google/gemma-3-1b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        model  minutes_taken  \\\n",
       "0    deepseek-ai.deepseek-r1-distill-qwen-32b           19.0   \n",
       "1                   mistralai/magistral-small            7.0   \n",
       "2    deepseek-ai.deepseek-r1-distill-qwen-14b            3.0   \n",
       "3              microsoft/phi-4-reasoning-plus            5.0   \n",
       "4                          google/gemma-3-12b            6.0   \n",
       "5      mistralai_-_mistral-nemo-instruct-2407            5.0   \n",
       "6                          qwen/qwen2.5-vl-7b            3.0   \n",
       "7                           google/gemma-2-9b            3.0   \n",
       "8                  meta-llama-3.1-8b-instruct            3.0   \n",
       "9    deepseek-ai.deepseek-r1-distill-llama-8b            3.0   \n",
       "10                deepseek-r1-distill-qwen-7b            2.0   \n",
       "11                              qwen/qwen3-8b            6.0   \n",
       "12         deepseek/deepseek-r1-0528-qwen3-8b            3.0   \n",
       "13         mistralai/mistral-7b-instruct-v0.3            3.0   \n",
       "14       mistralai_-_mistral-7b-instruct-v0.2            3.0   \n",
       "15                          google/gemma-3-4b            3.0   \n",
       "16             microsoft/phi-4-mini-reasoning            2.0   \n",
       "17  deepseek-ai.deepseek-r1-distill-qwen-1.5b            0.5   \n",
       "18                      llama-3.2-1b-instruct            0.5   \n",
       "19                          google/gemma-3-1b            0.5   \n",
       "\n",
       "    n_wrong_insights  n_wrong_obs  n_wrong_plans  completed_10_rounds  \\\n",
       "0                  3            1              2                 True   \n",
       "1                  0            0              0                 True   \n",
       "2                  7            5              0                 True   \n",
       "3                  1            1              3                False   \n",
       "4                  0            0              0                 True   \n",
       "5                  0            0              0                 True   \n",
       "6                  0            0              0                 True   \n",
       "7                  0            0              0                 True   \n",
       "8                  1            0              0                 True   \n",
       "9                  0            0              0                 True   \n",
       "10                 0            1              1                 True   \n",
       "11                 0            0              0                False   \n",
       "12                 0            0              0                 True   \n",
       "13                 0            0              0                 True   \n",
       "14                 5            0              8                 True   \n",
       "15                 0            0              0                 True   \n",
       "16                 0            0              2                False   \n",
       "17                -1           -1             -1                False   \n",
       "18                 0            0              8                False   \n",
       "19                 1            1              4                 True   \n",
       "\n",
       "    model_size_gb parameters  \n",
       "0            16.0        32B  \n",
       "1            13.5       None  \n",
       "2             9.0        14B  \n",
       "3             8.0       None  \n",
       "4             8.0       None  \n",
       "5             7.0        12B  \n",
       "6             6.0         7B  \n",
       "7             6.0         9B  \n",
       "8             5.0         8B  \n",
       "9             5.0         8B  \n",
       "10            5.0         7B  \n",
       "11            5.0       None  \n",
       "12            5.0       None  \n",
       "13            4.0         7B  \n",
       "14            4.0         7B  \n",
       "15            3.0       None  \n",
       "16            2.0       None  \n",
       "17            2.0       1.5B  \n",
       "18            1.0         1B  \n",
       "19            0.7       None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs_results = {\n",
    "    'model': ['deepseek-ai.deepseek-r1-distill-qwen-32b',\n",
    "                'mistralai/magistral-small',\n",
    "                'deepseek-ai.deepseek-r1-distill-qwen-14b',\n",
    "                'microsoft/phi-4-reasoning-plus',\n",
    "                'google/gemma-3-12b',\n",
    "                'mistralai_-_mistral-nemo-instruct-2407',\n",
    "                'qwen/qwen2.5-vl-7b',\n",
    "                'google/gemma-2-9b',\n",
    "                'meta-llama-3.1-8b-instruct',\n",
    "                'deepseek-ai.deepseek-r1-distill-llama-8b',\n",
    "                'deepseek-r1-distill-qwen-7b',\n",
    "                'qwen/qwen3-8b',\n",
    "                'deepseek/deepseek-r1-0528-qwen3-8b',\n",
    "                'mistralai/mistral-7b-instruct-v0.3',\n",
    "                'mistralai_-_mistral-7b-instruct-v0.2',\n",
    "                'google/gemma-3-4b',\n",
    "                'microsoft/phi-4-mini-reasoning',\n",
    "                'deepseek-ai.deepseek-r1-distill-qwen-1.5b',\n",
    "                'llama-3.2-1b-instruct',\n",
    "                'google/gemma-3-1b'],\n",
    "    'minutes_taken': [19, 7, 3, 5, 6, 5, 3, 3, 3, 3, 2, 6, 3, 3, 3, 3, 2, 0.5, 0.5, 0.5],\n",
    "    'n_wrong_insights': [3,0,7,1,0,0,0,0,1,0,0,0,0,0,5,0,0,-1,0,1],\n",
    "    'n_wrong_obs': [1,0,5,1,0,0,0,0,0,0,1,0,0,0,0,0,0,-1,0,1],\n",
    "    'n_wrong_plans': [2,0,0,3,0,0,0,0,0,0,1,0,0,0,8,0,2,-1,8,4],\n",
    "    'completed_10_rounds':[True, True, True, False,True, True, True,True, True, True,True, False,True, True, True,True, False,False, False, True],\n",
    "    'model_size_gb': [16,13.5,9,8,8,7,6,6,5,5,5,5,5,4,4,3,2,2,1,0.7],\n",
    "    'parameters': ['32B', None, \"14B\", None, None, \"12B\", \"7B\", \"9B\", \"8B\", \"8B\", \"7B\", None, None, \"7B\", \"7B\", None, None, \"1.5B\", \"1B\", None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(all_runs_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231ae86",
   "metadata": {},
   "source": [
    "# Mistral AI Inference - Performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f151c",
   "metadata": {},
   "source": [
    "Mistral AI API improves performance from 35\" per answer (locally) to 5\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"magistral-small-2506-API\"\n",
    "MODEL_MISTRAL = MODEL_NAME.replace(\"-API\",\"\")\n",
    "print(f\"Using model: {MODEL_MISTRAL}\")\n",
    "\n",
    "client = Mistral(api_key=API_KEY)\n",
    "\n",
    "paths = create_output_paths(MODEL_NAME )\n",
    "\n",
    "paths = create_output_paths(MODEL_NAME)\n",
    "plans, insights, market_data = \"No previous plans.\", \"No previous insights.\", \"No previous market data.\"\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "for i in range(1, 300+1):\n",
    "    prompt = GENERAL_PROMPT.format(\n",
    "                                marginal_cost=MG_C,\n",
    "                                willigness_to_pay=WILLIWGNES_TO_PAY,\n",
    "                                previous_plans = plans,\n",
    "                                previous_insights = insights,\n",
    "                                market_data = market_data\n",
    "                                )\n",
    "    \n",
    "    trial = 0\n",
    "\n",
    "    while trial <= 3:         \n",
    "        try:\n",
    "            chat_response = client.chat.complete(\n",
    "                model= MODEL_MISTRAL,\n",
    "                stream=False,\n",
    "                temperature=0.7,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                messages = [\n",
    "                { \"role\": \"system\", \"content\": PP_P0},\n",
    "                { \"role\": \"system\", \"content\": \"\"\"Respond only with a JSON object with this schema:\n",
    "                                    {\n",
    "                                    \"observations\": string,\n",
    "                                    \"plans\": string,\n",
    "                                    \"insights\": string,\n",
    "                                    \"chosen_price\": float\n",
    "                                    }\"\"\"\n",
    "                                    },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            )\n",
    "            result = json.loads(chat_response.choices[0].message.content)\n",
    "            if isinstance(result, list):\n",
    "                result = result[0]\n",
    "            \n",
    "            insights = result['insights']\n",
    "            observations = result['observations']\n",
    "            plans = result['plans']\n",
    "            price = result['chosen_price']\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to get response at round {i}: {e}\")\n",
    "            trial +=1\n",
    "        \n",
    "        finally:\n",
    "            if trial >= 3:\n",
    "                print(f\"[ERROR] Failed to get response at round {i}: {e}\")\n",
    "                break\n",
    "\n",
    "    quantity = get_quantities(p=tuple([price]),\n",
    "                                a0=A_0,\n",
    "                                a=A,\n",
    "                                mu=MU,\n",
    "                                alpha=ALPHA,\n",
    "                                multiplier=BETA,\n",
    "                                sigma=SIGMA,\n",
    "                                group_idxs=group_idxs,\n",
    "                            )\n",
    "    profit = get_profits(\n",
    "                            p=([price]),\n",
    "                            c=C,\n",
    "                            a0=A_0,\n",
    "                            a=A,\n",
    "                            mu=MU,\n",
    "                            alpha=ALPHA,\n",
    "                            multiplier=BETA,\n",
    "                            sigma=SIGMA,\n",
    "                            group_idxs=group_idxs,\n",
    "                        )\n",
    "\n",
    "\n",
    "    price_history.append(price)\n",
    "    quantity_history.append(quantity)\n",
    "    profit_history.append(profit)\n",
    "    time_history.append(i)\n",
    "\n",
    "    market_data_result = f\"\"\"Round {i}:\\n \\t - My price: {price}\\n \\t - Quantity sold: {quantity[0]}\\n \\t - My profit earned: {profit[0]}\\n\"\"\"\n",
    "    market_data = save_round_data(i, paths, insights, plans, observations, market_data_result)\n",
    "\n",
    "    #Update plot\n",
    "    update_plot(fig, axs, i, p_m, q_m, pi_m, price_history, quantity_history, \n",
    "                profit_history, time_history, MODEL_NAME, paths[\"start_time\"], paths[\"plot\"])\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6265a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_target = p_m[0]\n",
    "converged = has_converged_to_price(price_history, price_target)\n",
    "\n",
    "if converged:\n",
    "    print(f\"Prices converged to {price_target}\")\n",
    "else:\n",
    "    print(f\"Prices did NOT converge to {price_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df413c",
   "metadata": {},
   "source": [
    "# Results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3e69f",
   "metadata": {},
   "source": [
    "The models that passed the criteria at this stage are:\n",
    "- Deepseek R1 Distill QWEN 32B\n",
    "- MistralAI - Magistral Small\n",
    "- Google Gemma 2 9B\n",
    "- Deepseek R1 Qwen3 8B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f50e4",
   "metadata": {},
   "source": [
    "![Deepseek 32B](imgs/results_deepseek_r1_distill_qwen_32b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79995a57",
   "metadata": {},
   "source": [
    "![Deepseek 8B](imgs/results_deepseek_r1_qwen3_8b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534db47",
   "metadata": {},
   "source": [
    "![Gemma 2 9B](imgs/results_gemma_2_9b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edf1a3",
   "metadata": {},
   "source": [
    "![Magistral Small](imgs/results_magistral_small.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8214c9f",
   "metadata": {},
   "source": [
    "We will proceed using the MistralAI API to fasten experimentation as latency is highly reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5e585",
   "metadata": {},
   "source": [
    "![Magistral Small API](imgs/results_magistral_small_API.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
